{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Neural Netowrk for the MNIST Dataset\n",
    "\n",
    "See the original GAN [paper](https://arxiv.org/pdf/1406.2661.pdf) and this Open AI [blog post](https://openai.com/blog/generative-models/) related to this [article](https://arxiv.org/abs/1606.03498).\n",
    "\n",
    "In implementing this, I have used some of the recommendations given in the [DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "Finally, TensorFlow has recently added a [blog post](https://www.tensorflow.org/tutorials/generative/dcgan) on a simple DCGAN for the MNIST dataset. Interestingly, they are asppraoching this in a similar way that I have here. Admittedly, their `training_step` implementation is much better and cleaner than mine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from scipy.special import softmax\n",
    "\n",
    "import utilities as utils\n",
    "import models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = tf.keras.datasets.mnist.load_data()\n",
    "print(f\"\\tTrain set: {train_set[0].shape}, {train_set[1].shape}\")\n",
    "print(f\"\\tTest set:  {test_set[0].shape}, {test_set[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "Now that we have confidence in the generative and the discriminative models, we can proceed with our GAN setup. I will use these pretrained models to reduce the training time. For more details on GANs, see this [blog post](https://openai.com/blog/generative-models/) from OpenAI.\n",
    "\n",
    "First, I setup a new model that contains both the generative and the discriminative models. This new model is derived from `tf.keras.Model`, with a custom `call` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_rep_size = 40\n",
    "\n",
    "#tf.keras.backend.clear_session()\n",
    "gan_model = models.GANModel(\n",
    "    discriminator_model=models.construct_discriminator_model(),\n",
    "    generator_model=models.construct_generator_model(\n",
    "        input_size=hidden_rep_size, \n",
    "        output_activation=\"linear\",\n",
    "        with_batchnorm=True))\n",
    "\n",
    "# To build the model:\n",
    "gan_model((\n",
    "    tf.constant([True, False, True], dtype=tf.bool), \n",
    "    tf.zeros((3, 28, 28, 1), dtype=tf.float32), \n",
    "    tf.zeros((3, hidden_rep_size), dtype=tf.int32)\n",
    "))\n",
    "\n",
    "gan_model.summary(print_fn=(lambda *args: print(\"\\t\", *args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output of the Generator Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = softmax(\n",
    "    np.random.randn(10, hidden_rep_size), axis=1)\n",
    "generated_images = gan_model.generator_model.predict(generator_input)\n",
    "\n",
    "n = 1\n",
    "fig = plt.figure(figsize=(18., 2.))\n",
    "for image in generated_images:\n",
    "    ax = plt.subplot(1, 10, n)\n",
    "    normalized_image = np.clip(image, 0.0, 255.0).astype(np.uint8)\n",
    "    ax.imshow(normalized_image, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    n += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sgd_optimizer = False #True\n",
    "if use_sgd_optimizer:\n",
    "    optimizer = tf.keras.optimizers.SGD(\n",
    "        learning_rate=0.0001, momentum=0.0, nesterov=False, name=\"SGD\")\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001, beta_1=0.9, beta_2=0.999, \n",
    "        epsilon=1e-07, amsgrad=False, name=\"Adam\")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, name=\"crossentropy\")\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "\n",
    "gan_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Early stoppying callback:\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", \n",
    "    min_delta=0.0005, \n",
    "    patience=20, \n",
    "    verbose=1,\n",
    "    mode=\"min\", \n",
    "    baseline=None, \n",
    "    restore_best_weights=True)\n",
    "\n",
    "gan_seq_gen = utils.MNIST_GAN_Sequence_Generator(\n",
    "    train_set[0][..., np.newaxis].astype(np.float32), \n",
    "    batch_size=batch_size, \n",
    "    hidden_rep_size = hidden_rep_size,\n",
    "    temperature=None)\n",
    "\n",
    "# Fit model\n",
    "fit_history = gan_model.fit(\n",
    "    gan_seq_gen,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=None,\n",
    "    verbose=1,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    workers=8,\n",
    "    callbacks=[\n",
    "        early_stopping_callback\n",
    "    ])\n",
    "\n",
    "# epochs = np.arange(1, 1 + num_epochs)\n",
    "# plt.plot(epochs, fit_history.history[\"loss\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = softmax(\n",
    "    np.random.randn(10, hidden_rep_size), axis=1)\n",
    "generated_images = gan_model.generator_model.predict(generator_input)\n",
    "\n",
    "n = 1\n",
    "fig = plt.figure(figsize=(18., 2.))\n",
    "for image in generated_images:\n",
    "    ax = plt.subplot(1, 10, n)\n",
    "    normalized_image = np.clip(image, 0.0, 255.0).astype(np.uint8)\n",
    "    ax.imshow(normalized_image, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    n += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(gan_seq_gen) // 50):\n",
    "    x, y = gan_seq_gen[idx]\n",
    "    res = gan_model.predict(x)\n",
    "    print(f\"\\t[{idx}]:\\t{np.sum(y)}\\t{np.sum(np.argmax(res, axis=1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
